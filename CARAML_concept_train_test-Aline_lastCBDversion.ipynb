{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgnf42sYXRaq"
   },
   "source": [
    "## Machine locale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "EJ9SythE1xb1",
    "outputId": "3fee5688-91fd-458a-cc79-ce6faddc8451"
   },
   "outputs": [],
   "source": [
    "#cd /srv/Annif/anaconda3/bin\n",
    "#nohup ./jupyter-notebook --no-browser --port=8080 --NotebookApp.allow_origin='*' --NotebookApp.port_retries=0 --NotebookApp.token='toto'  --NotebookApp.disable_check_xsrf=True --NotebookApp.allow_remote_access=True &\n",
    "#lancer le serveur web pour la page app.py   :./uvicorn app:app --reload\n",
    "#uvicorn app:app --port 5000\n",
    "#cd /var/lib/docker/Annif/anaconda3/bin\n",
    "#lancer jupyter : nohup ./jupyter-notebook --no-browser --ip=10.34.202.165 --port=8080 --NotebookApp.allow_origin='*' --NotebookApp.port_retries=0 --NotebookApp.token='toto' --NotebookApp.allow_remote_access=True &\n",
    "#lancer les webservices : gunicorn --worker-class uvicorn.workers.UvicornWorker --bind '10.34.202.165:8000' --daemon app:app\n",
    "#installer des appli dans conda : ./conda install -c anaconda cx_Oracle\n",
    "import os\n",
    "path = \"/srv/Annif/CARAML\"\n",
    "os.chdir(path)\n",
    "os.listdir()\n",
    "data_path = path + \"\\\\data\"\n",
    "input_path = path + \"\\\\input\"\n",
    "output_path = path + \"\\\\output\"\n",
    "others_path = path + \"\\\\others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynEaXbGBlYFC"
   },
   "source": [
    "#Importation de librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0UyKyNAaonQ",
    "outputId": "202f74b3-da35-4396-869b-b15701b58b44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-24 12:14:36.796564: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-24 12:14:36.796690: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/runpy.py\", line 183, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib64/python3.6/runpy.py\", line 142, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/usr/lib64/python3.6/runpy.py\", line 109, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/spacy/__init__.py\", line 11, in <module>\n",
      "    from thinc.api import prefer_gpu, require_gpu, require_cpu  # noqa: F401\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/thinc/api.py\", line 2, in <module>\n",
      "    from .initializers import normal_init, uniform_init, glorot_uniform_init, zero_init\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/thinc/initializers.py\", line 4, in <module>\n",
      "    from .backends import Ops\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/thinc/backends/__init__.py\", line 7, in <module>\n",
      "    from .ops import Ops\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/thinc/backends/ops.py\", line 11, in <module>\n",
      "    from ..util import get_array_module, is_xp_array, to_numpy\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/thinc/util.py\", line 38, in <module>\n",
      "    import tensorflow.experimental.dlpack\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 41, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python.feature_column import feature_column_lib as feature_column\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_lib.py\", line 22, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import *\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\", line 147, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 20, in <module>\n",
      "    from tensorflow.python.keras.legacy_tf_layers import base\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import models\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/models.py\", line 20, in <module>\n",
      "    from tensorflow.python.keras import metrics as metrics_module\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\", line 34, in <module>\n",
      "    from tensorflow.python.keras import activations\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/activations.py\", line 18, in <module>\n",
      "    from tensorflow.python.keras.layers import advanced_activations\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/__init__.py\", line 29, in <module>\n",
      "    from tensorflow.python.keras.layers.preprocessing.image_preprocessing import CenterCrop\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py\", line 29, in <module>\n",
      "    from tensorflow.python.keras.preprocessing import image as image_preprocessing\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/preprocessing/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras.preprocessing import image\n",
      "  File \"/srv/Annif/.local/lib/python3.6/site-packages/tensorflow/python/keras/preprocessing/image.py\", line 39, in <module>\n",
      "    random_rotation = image.random_rotation\n",
      "AttributeError: module 'keras_preprocessing.image' has no attribute 'random_rotation'\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx8zqlhnzc2_",
    "outputId": "04725d5c-8e1d-45f3-d8bc-5ad55be450da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-multilearn in /var/lib/docker/Annif/.local/lib/python3.6/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Flask in /usr/local/lib/python3.6/site-packages (1.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/site-packages (from Flask) (2.11.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/site-packages (from Flask) (1.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/site-packages (from Flask) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/site-packages (from Flask) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib64/python3.6/site-packages (from Jinja2>=2.10.1->Flask) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjsB_bWTw_cv",
    "outputId": "55bf2dd4-0a73-481a-ebed-5ce801e7c115"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /srv/Annif/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /srv/Annif/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# librairies générales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#cbd 09102021\n",
    "import cx_Oracle\n",
    "import os \n",
    "\n",
    "# librairie affichage\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "#Pour le prétratiement\n",
    "import spacy\n",
    "import fr_core_news_sm\n",
    "#import en_core_web_sm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "#Tf Idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# OnevsRest\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#Encodage des labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "# Models\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "#Evaluation\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.metrics import roc_auc_score \n",
    "# from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "# from functools import partial\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "#Validation croisée\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "#from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "from collections import Counter\n",
    "\n",
    "seed=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cGoPh3A_ly0"
   },
   "source": [
    "# définition des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fvBQDAB8dq1"
   },
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "  dataset = pd.read_csv(path, converters={'titre': eval, 'resume': eval, 'codeD':eval})\n",
    "  return dataset\n",
    "# sauvegarder un dataframe en csv\n",
    "def save_dataset_to_csv(df, filename):\n",
    "  path_destination = 'save/'+ filename + '.csv'\n",
    "  df.to_csv(path_destination, index=False)\n",
    "#Importer le dataset préparé\n",
    "def get_dataset_init(path):\n",
    "  dataset_init = pd.read_csv(path, converters={'codeD': eval}) \n",
    "  return dataset_init\n",
    "# Lemmatiser les mots (déterminer le lemme) avec Spacy\n",
    "def lemmatisation(df, lang, save = True):\n",
    "  df_lem = df.copy()\n",
    "  if lang == 'french':\n",
    "    nlp = fr_core_news_sm.load(disable=[\"parser\", \"ner\"])\n",
    "  if lang == 'english':\n",
    "    nlp = en_core_web_sm.load(disable=[\"parser\", \"ner\"])\n",
    "  df_lem['titre'] = df_lem['titre'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n",
    "  df_lem['resume'] = df_lem['resume'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n",
    "  if save:\n",
    "    save_dataset_to_csv(df_lem, 'dataset_lem')\n",
    "  return df_lem\n",
    "\n",
    "# Transformer les phrases en sequences de mot avec word_tokenize.\n",
    "def tokenizer(df, lang):\n",
    "  df_token = df.copy()\n",
    "  df_token.titre = df_token.titre.map(lambda e: word_tokenize(e, language = lang))\n",
    "  df_token.resume = df_token.resume.map(lambda e: word_tokenize(e, language = lang))\n",
    "  return df_token\n",
    "\n",
    "#Les prétraitement suivants permettent de supprimer les ponctuations, de supprimer des stopwords et la lémmatisation. \n",
    "#Suppresion des ponctuations (caractères spéciaux)\n",
    "def funcPonct(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#Suppression des tokens numeriques\n",
    "def funcDigit(tokens):\n",
    "     tokens_p = [word for word in tokens if not word.isdigit()]\n",
    "     return tokens_p\n",
    "\n",
    "#Suppresion des stop words \n",
    "def funcSW(tokens, lang):\n",
    "    if lang == 'french':\n",
    "      stop_words = set(stopwords.words(lang)).union(fr_stop) #concatenation of two french stopword lists\n",
    "    if lang == 'english':\n",
    "      stop_words = set(stopwords.words(lang)).union(en_stop) #concatenation of two english stopword lists\n",
    "    tokens_sw = [w for w in tokens if not w in stop_words]\n",
    "    return tokens_sw\n",
    "    \n",
    "#Encodage des données, les accents seront enlevés\n",
    "def funcEnc(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ASCII', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#Stemmatisation (réduire les mots à leurs racines)\n",
    "def funcSTM(tokens, lang):\n",
    "    stemmer = SnowballStemmer(language=lang)\n",
    "    tokens_stm = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens_stm\n",
    "\n",
    "#Regrouper les fonctions\n",
    "def normalize(tokens, lang):\n",
    "    tokens = funcPonct(tokens)\n",
    "    tokens = funcDigit(tokens)\n",
    "    tokens = funcSW(tokens, lang)\n",
    "    tokens = funcEnc(tokens)\n",
    "    #tokens = funcSTM(tokens, lang)\n",
    "    return tokens\n",
    "\n",
    "#Fonction qui permet d'effectuer tout les pré-traitements au dataset passé en argument\n",
    "def preprocessing(df, lang, savename, save = False):\n",
    "  df_pre = df.copy()\n",
    "  df_pre = lemmatisation(df_pre, lang)\n",
    "  df_pre = tokenizer(df_pre, lang)\n",
    "  df_pre.titre = df_pre.titre.map(lambda e: normalize(e, lang))\n",
    "  df_pre.resume = df_pre.resume.map(lambda e: normalize(e, lang))\n",
    "  if save:\n",
    "    save_dataset_to_csv(df_pre, savename)\n",
    "  return df_pre\n",
    "# #Convertir liste en set\n",
    "# def convert_to_tuple(list):\n",
    "#   return tuple(list)\n",
    "\n",
    "#Encoder les labels\n",
    "def encode_label(y,ledomaine,save):\n",
    "  # remove duplicates from list of idC, then convert the list to string\n",
    "  y = y.map(np.unique).apply(lambda x: ','.join(map(str, x)))\n",
    "  #Transformer les labels en dictionnaire : par ex [300, 640] devient {300, 640}\n",
    "  y = [set(i.split(',')) for i in y]\n",
    "  #Encodage des labels\n",
    "  ## on le reinitialise\n",
    "  mlb = MultiLabelBinarizer()\n",
    "  y_encoded = mlb.fit_transform(y)\n",
    "  classes = mlb.classes_\n",
    "  if save:\n",
    "      pickle.dump(mlb, open(\"mlb_\"+ledomaine+\".pickle\", \"wb\"))\n",
    "  return y_encoded, classes\n",
    "\n",
    "#Iterative Splitting pour multilabel\n",
    "def iterative_train_test_split_dataframe(X, y, test_size):\n",
    "    df_index = np.expand_dims(X.index.to_numpy(), axis=1)\n",
    "    X_train, y_train, X_test, y_test = iterative_train_test_split(df_index, y, test_size = test_size)\n",
    "    X_train = X.loc[X_train[:,0]]\n",
    "    X_test = X.loc[X_test[:,0]]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "#splitting, encoding\n",
    "def splitting_encoding(df, col_label, test_size,ledomaine, save = True):\n",
    "  df_traited = df.copy()\n",
    "\n",
    "  # concatenate tokens\n",
    "  df_traited['titre'] = df_traited.titre.map(lambda e: ' '.join(e))\n",
    "  df_traited['resume'] = df_traited.resume.map(lambda e: ' '.join(e)) \n",
    "  df_traited['corpus'] = df_traited['titre'] + df_traited['resume']\n",
    "\n",
    "  # define X and y\n",
    "  X = df_traited.drop(columns=['idB','titre','resume',col_label])\n",
    "  y = df_traited[col_label]\n",
    "\n",
    "  #encode labels\n",
    "  encoding = encode_label(y,ledomaine, save )\n",
    "  y = encoding[0]\n",
    "  classes = encoding[1]\n",
    "\n",
    "  #Iterative splitting\n",
    "  X_train_txt, y_train, X_test_txt, y_test = iterative_train_test_split_dataframe(X, y, test_size = test_size)\n",
    "\n",
    "  return X_train_txt, y_train, X_test_txt, y_test, classes\n",
    "\n",
    "def vectorizer_tfidf(X_train, X_test, max_df, min_df, max_features, save = True):\n",
    "  regex_pattern = r'\\w{3,}' #des mots qui ont plus de trois caractères\n",
    "  vectorizer = TfidfVectorizer(max_df = max_df, min_df = min_df, max_features=max_features, ngram_range=(1,2), token_pattern=regex_pattern)\n",
    "  X_train = vectorizer.fit_transform(X_train.corpus)\n",
    "  X_test = vectorizer.transform(X_test.corpus)\n",
    "  features = vectorizer.get_feature_names()\n",
    "  if save:\n",
    "    pickle.dump(vectorizer, open(\"tfidf_domaine.pickle\", \"wb\"))\n",
    "  return X_train, X_test, features\n",
    "\n",
    "def encoding(df, col_label, ledomaine, save = True):\n",
    "  df_traited = df.copy()\n",
    "\n",
    "  # concatenate tokens\n",
    "  df_traited['titre'] = df_traited.titre.map(lambda e: ' '.join(e))\n",
    "  df_traited['resume'] = df_traited.resume.map(lambda e: ' '.join(e)) \n",
    "  df_traited['corpus'] = df_traited['titre'] + df_traited['resume']\n",
    "\n",
    "  # define X and y\n",
    "  X = df_traited.drop(columns=['idB','titre','resume',col_label])\n",
    "  y = df_traited[col_label]\n",
    "\n",
    "  #encode labels\n",
    "  encoding = encode_label(y,ledomaine, save )\n",
    "  y = encoding[0]\n",
    "  classes = encoding[1]\n",
    "\n",
    "  return X, y, classes\n",
    "\n",
    "def get_concepts_by_domaine(ledomaine, path_dataset, path_conc_dom):\n",
    "  # lire des fichiers necessaires pour construire le dataset\n",
    "  dataset_preprocessed = get_dataset(path_dataset)\n",
    "  idB_idC_codeD_g = pd.read_csv(path_conc_dom)\n",
    "\n",
    "  # selectionner les documents et les concepts dont le domaine correspond au premier argument 'domaine'\n",
    "  G = idB_idC_codeD_g.loc[idB_idC_codeD_g['newCodeD'] == ledomaine]\n",
    "  tmp = pd.merge(dataset_preprocessed, G, on = 'idB')\n",
    "  # supprimer les documents dont la fréquence de concept dans l'ensemble est inferieur à 3\n",
    "  tmp = tmp[tmp.groupby('idC').idB.transform(len) > 3]\n",
    "  tmp_grouped = tmp.groupby(['idB'])['idC'].aggregate(lambda x: tuple(x))\n",
    "  df_G = pd.merge(dataset_preprocessed, tmp_grouped, on ='idB').drop(columns='codeD')\n",
    "  return df_G\n",
    "#Sauvegarder le model \n",
    "def save_model(model, modelname, suffix):\n",
    "  if suffix == 'domaine':\n",
    "    joblib.dump(model, 'models/'+ modelname + '_' + suffix + '.pkl') \n",
    "  else:\n",
    "    joblib.dump(model, 'models/concept/'+ modelname + '_' + suffix + '.pkl') \n",
    "\n",
    "#Entraînement du classifieur passé en argument\n",
    "def trainning_br(df, X_train, y_train, algo, suffix, save = True):\n",
    "  if algo == 'lr':\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=10000, class_weight='balanced')\n",
    "  elif algo == 'svc':\n",
    "    model = LinearSVC(max_iter = 10000, class_weight='balanced')\n",
    "  elif algo == 'nb':\n",
    "    model = MultinomialNB()\n",
    "  elif algo == 'knn':\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "  elif algo == 'rf':\n",
    "    model = RandomForestClassifier(random_state=seed, class_weight='balanced')\n",
    "  else :\n",
    "    print('The algo ' + algo + ' is not defined!')\n",
    "\n",
    "  classifier = BinaryRelevance(\n",
    "    classifier = model\n",
    "  )\n",
    "  \n",
    "  %time classifier.fit(X_train, y_train)#Training the model on dataset\n",
    "  if save:\n",
    "    save_model(classifier, algo, suffix)\n",
    "    \n",
    "#connexion base oracle\n",
    "dsn_tns = cx_Oracle.makedsn('orpins-p-scan.v110.abes.fr', '1521', service_name='APISUDOC')\n",
    "\n",
    "def oracle_connect():\n",
    "    conn = cx_Oracle.connect(user=r'autorites', password='autorites0ieuf1', dsn=dsn_tns)\n",
    "    return conn\n",
    "def oracle_disconnect():\n",
    "    conn.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing des datas et sauvegarde (normalement a ne plus refaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p2Mww3F7c-RE"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-02c4ccd02c2a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-02c4ccd02c2a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ligne a commenter\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ligne a commenter\n",
    "df = get_dataset_init('input/dataset_final_g.csv')\n",
    "#df\n",
    "# formate les entrees\n",
    "preprocessing(df, 'french', 'dataset_preprocessed')\n",
    "#preprocessing(df, 'english', dataset_preprocessed_eng')  #Exmple de l'utilisation de la fonction si le dataset est en anglais "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8k9YW7bk6Yv"
   },
   "source": [
    "# Entraînement des modèles (ne plus faire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXvByC42mhwb"
   },
   "source": [
    "### Traitement entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FO0huWZj2LFI"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-0b3ad79309f9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-0b3ad79309f9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    entrainement sur tous les domaines pour les concepts (commenter cette ligne)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "entrainement sur tous les domaines pour les concepts (commenter cette ligne)\n",
    "new_data_pred = pickle.load(open(\"pred_domaine.dmp\", 'rb'))\n",
    "new_data_pred.prediction = new_data_pred.prediction.apply(list)\n",
    "\n",
    "new_data_pred = new_data_pred.explode('prediction')\n",
    "new_data = pd.read_csv('evaluation/edp_titres_resumes_fr_1.csv')\n",
    "new_data_preprocessed = preprocessing(new_data, 'french', 'test')\n",
    "new_classes = new_data_pred.prediction.unique()\n",
    "pos=0\n",
    "for i in new_classes:\n",
    "  pos=pos+1\n",
    "  if pos<50:\n",
    "        if i is no  t None:\n",
    "                if not os.path.isfile('tfidf_' + str(i) + '.pickle') or not os.path.isfile(\"models/concept/svc_concept_\" + str(i) + \".pkl\") or not os.path.isfile(\"mlb_\"+str(i)+\".pickle\"):\n",
    "                      print(str(i))\n",
    "                      df_G = get_concepts_by_domaine(str(i), 'save/dataset_preprocessed.csv', 'input/idB_idC_codeD_g.csv')\n",
    "\n",
    "                      # Copy du dataset dans la variable 'tmp' \n",
    "                      tmp = df_G.copy()\n",
    "\n",
    "                      # Splitting et encoding\n",
    "                      trait = splitting_encoding(tmp, 'idC', 0.3,str(i), save = True)\n",
    "                      X_train_txt = trait[0]\n",
    "                      y_train = trait[1]\n",
    "                      X_test_txt = trait[2]\n",
    "                      y_test = trait[3]\n",
    "                      #classes = trait[4]\n",
    "\n",
    "                      # TF-IDF Vectorizer\n",
    "                      regex_pattern = r'\\w{3,}' #des mots qui ont plus de trois caractères\n",
    "                      vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 5, max_features=5000, ngram_range=(1,2), token_pattern=regex_pattern)\n",
    "                      X_train = vectorizer.fit_transform(X_train_txt.corpus)\n",
    "                      X_test = vectorizer.transform(X_test_txt.corpus)\n",
    "                      #features = vectorizer.get_feature_names()\n",
    "                      ## save tf-idf vectorizer\n",
    "                      vectorizer_name = \"tfidf_\" + str(i) + '.pickle'\n",
    "                      pickle.dump(vectorizer, open(vectorizer_name, \"wb\"))\n",
    "\n",
    "                      # Entrainement des models puis la sauvegarde\n",
    "                      suffixe = 'concept_' + str(i)\n",
    "                      trainning_br(tmp, X_train, y_train, 'svc', suffixe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_DYOC_zFGf3"
   },
   "source": [
    "###Entrainement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOdROxG9KHZj"
   },
   "source": [
    "# Prédiction des concepts sur les données de test pour un domaine precis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNxP7AT7KhGQ"
   },
   "outputs": [],
   "source": [
    "domaine = 'G500'\n",
    "df = get_concepts_by_domaine(domaine, 'save/dataset_preprocessed.csv', 'input/idB_idC_codeD_g.csv')\n",
    "df\n",
    "# Copy du dataset dans la variable 'tmp' \n",
    "tmp = df.copy()\n",
    "tmp =df.head(2)\n",
    "#print(tmp)\n",
    "# Splitting et encoding\n",
    "trait = encoding(tmp, 'idC', domaine, save = False)\n",
    "X_test_txt = trait[0]\n",
    "classes = trait[2]\n",
    "vect_name = 'tfidf_' + domaine + '.pickle'\n",
    "vectorizer = pickle.load(open(vect_name, 'rb'))\n",
    "\n",
    "X_test = vectorizer.transform(X_test_txt.corpus)\n",
    "model = joblib.load(\"models/concept/svc_concept_\" + domaine + \".pkl\")\n",
    "y_pred = model.predict(X_test)\n",
    "#Afficher les résultats de prédiction\n",
    "mlb = pickle.load(open(\"mlb_\"+domaine+\".pickle\", 'rb'))\n",
    "y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "\n",
    "for item,label in zip(X_test_txt.corpus, y_pred_inversed):\n",
    "    print ('%s => %s' % (item[0:15],label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction des concepts sur les ppn de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260746, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_dataset('save/dataset_preprocessed.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idB</th>\n",
       "      <th>titre</th>\n",
       "      <th>resume</th>\n",
       "      <th>codeD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67292</th>\n",
       "      <td>053669754</td>\n",
       "      <td>[protection, tiers, face, association, contrib...</td>\n",
       "      <td>[loi, reconner, liberte, public, association, ...</td>\n",
       "      <td>[G320, G320, G100, G320, G300, G320, G320, G320]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246735</th>\n",
       "      <td>23758610X</td>\n",
       "      <td>[lapport, logiciel, simulation, algorithme, pr...</td>\n",
       "      <td>[these, objet, etude, apport, introduction, te...</td>\n",
       "      <td>[G510, G621, G510, G621, G510, G370, G621, G37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41599</th>\n",
       "      <td>043742459</td>\n",
       "      <td>[etude, vieillissement, fragilisation, acier, ...</td>\n",
       "      <td>[presente, systeme, indentation, instrumenter,...</td>\n",
       "      <td>[G600, G600, G530, G600, G530]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159856</th>\n",
       "      <td>158764986</td>\n",
       "      <td>[vitroceramique, nano, structurer, sio2sno2, f...</td>\n",
       "      <td>[avenement, source, lumineuse, performant, com...</td>\n",
       "      <td>[G600, G530, G530, G540, G600, G530, G540, G54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219082</th>\n",
       "      <td>199755973</td>\n",
       "      <td>[modelisation, dimensionnemer, optimisation, c...</td>\n",
       "      <td>[annee, chiffre, insecurite, routier, montrer,...</td>\n",
       "      <td>[G600, G320, G320, G793, G600, G500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98398</th>\n",
       "      <td>087501430</td>\n",
       "      <td>[spectroscopie, resoudre, temps, laser, blanc,...</td>\n",
       "      <td>[principal, chromophore, endogene, interet, tr...</td>\n",
       "      <td>[G530, G570, G530]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174014</th>\n",
       "      <td>169770877</td>\n",
       "      <td>[synthese, analogue, aminoacyl, arnt, conjugue...</td>\n",
       "      <td>[aminoacyl, transferase, fem, catalysent, etap...</td>\n",
       "      <td>[G570, G570, G570]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211611</th>\n",
       "      <td>194055248</td>\n",
       "      <td>[modele, resolution, approche, efficace, probl...</td>\n",
       "      <td>[these, interesse, resolution, probleme, optim...</td>\n",
       "      <td>[G510, G510, G621, G510, G510, G621, G510]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13076</th>\n",
       "      <td>01181568X</td>\n",
       "      <td>[genetique, population, tilapias, interet, aqu...</td>\n",
       "      <td>[etude, variabilite, genetique, espece, tilapi...</td>\n",
       "      <td>[G570, G570, G540, G570, G570, G910, G570, G63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158096</th>\n",
       "      <td>157713725</td>\n",
       "      <td>[microbiote, intestinal, nouveau, naitre, prem...</td>\n",
       "      <td>[nouveau, naitre, premature, implantation, mic...</td>\n",
       "      <td>[G570, G570, G610, G300, G610, G610, G570, G61...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              idB                                              titre                                             resume  \\\n",
       "67292   053669754  [protection, tiers, face, association, contrib...  [loi, reconner, liberte, public, association, ...   \n",
       "246735  23758610X  [lapport, logiciel, simulation, algorithme, pr...  [these, objet, etude, apport, introduction, te...   \n",
       "41599   043742459  [etude, vieillissement, fragilisation, acier, ...  [presente, systeme, indentation, instrumenter,...   \n",
       "159856  158764986  [vitroceramique, nano, structurer, sio2sno2, f...  [avenement, source, lumineuse, performant, com...   \n",
       "219082  199755973  [modelisation, dimensionnemer, optimisation, c...  [annee, chiffre, insecurite, routier, montrer,...   \n",
       "98398   087501430  [spectroscopie, resoudre, temps, laser, blanc,...  [principal, chromophore, endogene, interet, tr...   \n",
       "174014  169770877  [synthese, analogue, aminoacyl, arnt, conjugue...  [aminoacyl, transferase, fem, catalysent, etap...   \n",
       "211611  194055248  [modele, resolution, approche, efficace, probl...  [these, interesse, resolution, probleme, optim...   \n",
       "13076   01181568X  [genetique, population, tilapias, interet, aqu...  [etude, variabilite, genetique, espece, tilapi...   \n",
       "158096  157713725  [microbiote, intestinal, nouveau, naitre, prem...  [nouveau, naitre, premature, implantation, mic...   \n",
       "\n",
       "                                                    codeD  \n",
       "67292    [G320, G320, G100, G320, G300, G320, G320, G320]  \n",
       "246735  [G510, G621, G510, G621, G510, G370, G621, G37...  \n",
       "41599                      [G600, G600, G530, G600, G530]  \n",
       "159856  [G600, G530, G530, G540, G600, G530, G540, G54...  \n",
       "219082               [G600, G320, G320, G793, G600, G500]  \n",
       "98398                                  [G530, G570, G530]  \n",
       "174014                                 [G570, G570, G570]  \n",
       "211611         [G510, G510, G621, G510, G510, G621, G510]  \n",
       "13076   [G570, G570, G540, G570, G570, G910, G570, G63...  \n",
       "158096  [G570, G570, G610, G300, G610, G610, G570, G61...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prediction pour un domaine precis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-d0b16429b9cb>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['titre'] = df_test.titre.map(lambda e: ' '.join(e))\n",
      "<ipython-input-14-d0b16429b9cb>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['resume'] = df_test.resume.map(lambda e: ' '.join(e))\n",
      "<ipython-input-14-d0b16429b9cb>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['corpus'] = df_test['titre'] + df_test['resume']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelisation th => ('027253597', '027253783', '027289850', '02735735X', '027392198')\n",
      "nature genes pr => ('027242722', '027246183', '030900182')\n"
     ]
    }
   ],
   "source": [
    "#test sur un domaine precis\n",
    "domaine = 'G550'\n",
    "\n",
    "list_ppn = ['001976508', '001149733']\n",
    "df_test=df[df.idB.isin(list_ppn)]\n",
    "df_test['titre'] = df_test.titre.map(lambda e: ' '.join(e))\n",
    "df_test['resume'] = df_test.resume.map(lambda e: ' '.join(e)) \n",
    "df_test['corpus'] = df_test['titre'] + df_test['resume']\n",
    "\n",
    "\n",
    "vectorizer = pickle.load(open('tfidf_' + domaine + '.pickle', 'rb'))\n",
    "\n",
    "X_test = vectorizer.transform(df_test.corpus)\n",
    "model = joblib.load(\"models/concept/svc_concept_\" + domaine + \".pkl\")\n",
    "y_pred = model.predict(X_test)\n",
    "#Afficher les résultats de prédiction\n",
    "mlb = pickle.load(open(\"mlb_\"+domaine+\".pickle\", 'rb'))\n",
    "y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "\n",
    "for item,label in zip(df_test.corpus, y_pred_inversed):\n",
    "    print ('%s => %s' % (item[0:15],label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sur tous les domaines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G300\n",
      "G650\n",
      "G320\n",
      "G910\n",
      "G200\n",
      "G621\n",
      "G700\n",
      "G900\n",
      "G400\n",
      "G500\n",
      "G520\n",
      "G550\n",
      "G510\n",
      "G530\n",
      "G690\n",
      "G610\n",
      "G560\n",
      "G630\n",
      "G640\n",
      "G540\n",
      "G600\n",
      "G800\n",
      "G150\n",
      "G793\n",
      "G580\n",
      "G577\n",
      "G370\n",
      "G100\n",
      "         idB indexeur       auto algo\n",
      "0  002731401      svc  027238563  svc\n",
      "1  002731401      svc  02723858X  svc\n",
      "2  002882418      svc  027690342  svc\n",
      "3  003510972      svc  027791246  svc\n",
      "4  003888908      svc  027286460  svc\n",
      "5  00457382X      svc  02722550X  svc\n",
      "6  00457382X      svc  027790088  svc\n",
      "7  004574230      svc  027791246  svc\n",
      "8  004576675      svc  027261697  svc\n",
      "9  004617452      svc  049647490  svc\n",
      "         idB                                               auto\n",
      "0  000217549                         Technique de la production\n",
      "1  00212629X                                    Paléogéographie\n",
      "2  002587432    Politique et gouvernement,Relations extérieures\n",
      "3  002632489                                 Droit,Lutte contre\n",
      "4  002731401                  Mariage,Mariage,Biens incorporels\n",
      "5  002736098  Division internationale du travail,Politique i...\n",
      "6  002823993       Tradition orale,Analyse du discours narratif\n",
      "7  002834391                                            Énergie\n",
      "8  002882418               Communication dans les organisations\n",
      "9  003209792                          Politique et gouvernement\n"
     ]
    }
   ],
   "source": [
    "#test sur tous les domaines\n",
    "#recherche les domaines\n",
    "#attention le 570 n a pas pu etre entrainé car pas assez de données\n",
    "#domaine = 'G600'\n",
    "domaine = ''\n",
    "test=True\n",
    "test_size=20000\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "list_ppn = ['004762487','005799066','007114621','009380493','011374403','012802689','041456696','041607511','042051266','042415926',\n",
    "'043801978','043817149','049605976','058637699','059991771','060011165','063462087','067350259','071024166','074137271','078393302',\n",
    "'083418113','092418945','095140913','109365291','111877016','117378135','120935031','131590553','132481251','138462127','139250905',\n",
    "'140356533','150214847','152903356','158308123','160615941','161860044','164275193','166019127','166968765','167357921','168875713',\n",
    "'170131521','172397448','175120447','175707715','178106062','178644463','179799827','180076388','181027356','181848848','183381750',\n",
    "'185959857','187372837','188126376','188572953','190619694','191269816','196498260','196622883','198270836','202377911','203350073',\n",
    "'219502250','220219141','223744824','224225146','224824023','225564181','225872048','226775704','227234642','229279023','231745265',\n",
    "'232378169','232446369','236856871','241151821','242216595','242325602','244376476','244791252','250153025','252824881','11083609X',\n",
    "'14295456X','16054159X','17080366X','18742070X','22341302X','22348153X','23713392X','24243066X','25417695X']\n",
    "\n",
    "df = get_dataset('save/dataset_preprocessed.csv')\n",
    "conn=oracle_connect()\n",
    "if test:\n",
    "    # pour le faire sur tous les ppn de test\n",
    "    dataset_rec = pd.read_csv('/srv/Annif/CARAML/save/ppn_test.csv',usecols=[\"idB\"])\n",
    "    dataset_test=dataset_rec.head(test_size)\n",
    "    df_test=df[df.idB.isin(dataset_test.idB)]\n",
    "else:\n",
    "    ### sinon sur la liste a commenter dans l'autre cas\n",
    "    \n",
    "    ##df.shape\n",
    "    df_test=df[df.idB.isin(list_ppn)]\n",
    "\n",
    "df_test.titre = df_test.titre.map(lambda e: ' '.join(e))\n",
    "df_test.resume = df_test.resume.map(lambda e: ' '.join(e)) \n",
    "df_test['corpus'] = df_test.titre + df_test.resume\n",
    "\n",
    "\n",
    "#df_test.corpus\n",
    "model = joblib.load(\"models/svc_domaine.pkl\")\n",
    "mlb = pickle.load(open(\"mlb_domaine.pickle\", 'rb'))\n",
    "vectorizer = pickle.load(open('tfidf_domaine.pickle', 'rb'))\n",
    "X_test_vec = vectorizer.transform(df_test.corpus)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "#Afficher les résultats de prédiction\n",
    "y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "#for item,label in zip(df_test.idB, y_pred_inversed):\n",
    "#    print ('%s => %s' % (item[0:15],label))\n",
    "df_resu_ppn = pd.DataFrame(columns=['idB','indexeur','auto','algo'])\n",
    "#df_resu_ppn_lst = pd.DataFrame(columns=['Idb','ppn_concept'])\n",
    "df_resu = pd.DataFrame(columns=['idB','auto'])\n",
    "\n",
    "df_test['prediction'] = pd.Series(y_pred_inversed, index=df_test.index)\n",
    "new_data_pred = df_test.explode('prediction')\n",
    "#new_data_pred\n",
    "new_classes = new_data_pred.prediction.unique()\n",
    "if domaine!='' :\n",
    "    new_classes = [domaine]\n",
    "\n",
    "for i in new_classes:\n",
    "    if os.path.isfile('tfidf_' + str(i) + '.pickle') and os.path.isfile(\"models/concept/svc_concept_\" + str(i) + \".pkl\") and os.path.isfile(\"mlb_\"+str(i)+\".pickle\"):\n",
    "        print(str(i))\n",
    "        dataset1 = new_data_pred.loc[new_data_pred['prediction'] == i]\n",
    "        \n",
    "        dataset = dataset1.copy()\n",
    "        dataset.titre = dataset.titre.map(lambda e: word_tokenize(e, language = 'french'))\n",
    "        dataset.resume = dataset.resume.map(lambda e: word_tokenize(e, language = 'french'))\n",
    "        #dataset=preprocessing(dataset1, 'french', 'test')       \n",
    "        # concatenate tokens\n",
    "        #dataset['titre'] = dataset.titre.map(lambda e: ' '.join(e))\n",
    "        dataset.titre = dataset.titre.map(lambda e: ' '.join(e))\n",
    "        dataset.resume = dataset.resume.map(lambda e: ' '.join(e)) \n",
    "        dataset['corpus'] = dataset['titre']  +' ' + dataset['resume']\n",
    "        #print(dataset.corpus)\n",
    "        vectorizer = pickle.load(open('tfidf_' + str(i) + '.pickle', 'rb'))\n",
    "        new_X = vectorizer.transform(dataset.corpus)\n",
    "        model = joblib.load(\"models/concept/svc_concept_\" + str(i) + \".pkl\")\n",
    "        pred = model.predict(new_X)\n",
    "        mlb = pickle.load(open(\"mlb_\"+str(i)+\".pickle\", 'rb'))\n",
    "        y_pred_inversed = mlb.inverse_transform(pred)\n",
    "        \n",
    "        for item,label in zip(dataset.idB, y_pred_inversed):\n",
    "            for x in label:\n",
    "                #print ('  %s => %s' % (item[0:200],label))\n",
    "                df_resu_ppn = df_resu_ppn.append({'idB': item,'algo': 'svc','auto':x,'indexeur':'svc'}, ignore_index=True)\n",
    "                #optionnel\n",
    "                \n",
    "                c = conn.cursor()\n",
    "                c.execute(\"select datas from autorites.AUT_2XX where tag='250$a' and ppn='\"+x+\"'\")\n",
    "                for row in c:\n",
    "                    #print (row[0]) \n",
    "                    #print ('      %s => %s => %s' % (item[0:200],x,row[0]))\n",
    "                    df_resu = df_resu.append({'idB': item,'auto':row[0]}, ignore_index=True)\n",
    "                    \n",
    "                c.close()\n",
    "oracle_disconnect()\n",
    "df_resu.sort_values(by=['idB'])\n",
    "df_resu_ppn.sort_values(by=['idB'])\n",
    "df_resu=df_resu.groupby(['idB'])['auto'].apply(','.join).reset_index()\n",
    "\n",
    "df_resu.to_excel(\"output/prediction_concepts_newdata_aline.xlsx\") \n",
    "\n",
    "                                 \n",
    "df_resu_ppn.to_excel(\"evaluation_support/indexation_auto/auto.xlsx\")\n",
    "print(df_resu_ppn.head(10))\n",
    "df_resu_ppn=df_resu_ppn.drop('algo', 1)\n",
    "df_resu_ppn=df_resu_ppn.groupby(['idB','indexeur'])['auto'].apply(','.join).reset_index()\n",
    "df_resu_ppn.indexeur = 'indexeur 1'\n",
    "df_resu_ppn_csv=pd.concat([df_resu_ppn['idB'],df_resu_ppn['indexeur'],df_resu_ppn['auto'].str.split(',',n=20,expand=True)], axis = 1)\n",
    "df_resu_ppn_csv.to_csv(\"evaluation_support/indexation_indexeurs/indexeur1.csv\", index=False)\n",
    "pd.set_option('display.width', 800)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(df_resu.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000217549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>001976508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idB\n",
       "0  000217549\n",
       "1  000217549\n",
       "2  000217549\n",
       "3  000217549\n",
       "4  000217549\n",
       "5  000217549\n",
       "6  000217549\n",
       "7  000217549\n",
       "8  000217549\n",
       "9  001976508"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_rec.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7nweNyP8b-S"
   },
   "source": [
    "# Prédiction des concepts sur des nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "QEY9XSWuaMbO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G530\n",
      "G610\n",
      "G800\n",
      "G320\n",
      "G900\n",
      "G100\n",
      "G200\n",
      "G510\n",
      "G640\n",
      "G700\n",
      "G600\n",
      "G650\n",
      "                                               titre                                            concept\n",
      "0  Diabète en prison : mise en place d’un program...                                            Prisons\n",
      "1  Impact d’un programme d’éducation thérapeutiqu...  Éducation des patients,Patients,Évaluation,Pat...\n",
      "2  L’autonomie du patient, d’une sémantique uniqu...                 Relations médecin-patient,Patients\n",
      "3  Similitudes, différences, complémentarités ent...                                          Recherche\n",
      "4  Usage du jeu vidéo pour l’éducation thérapeuti...  Éducation des patients,Emploi en thérapeutique...\n"
     ]
    }
   ],
   "source": [
    "#recherche les domaines\n",
    "#attention le 570 n a pas pu etre entrainé car pas assez de données\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "df = pd.DataFrame(columns=['titre','concept'])\n",
    "\n",
    "\n",
    "\n",
    "new_data = pd.read_csv('evaluation/edp_titres_resumes_fr_1.csv')\n",
    "new_data=new_data.head(10)\n",
    "new_data_preprocessed = preprocessing(new_data, 'french', 'test')\n",
    "new_data_preprocessed['idB'] = new_data.idB.map(lambda e: word_tokenize(e, language = 'french'))\n",
    "new_data_preprocessed['origine'] = new_data.titre.astype(str)\n",
    "new_data_preprocessed['titre'] = new_data_preprocessed.titre.map(lambda e: ' '.join(e))\n",
    "new_data_preprocessed['resume'] = new_data_preprocessed.resume.map(lambda e: ' '.join(e)) \n",
    "new_data_preprocessed['corpus'] = new_data_preprocessed['titre'] + new_data_preprocessed['resume']\n",
    "#new_data_preprocessed.corpus\n",
    "model = joblib.load(\"models/svc_domaine.pkl\")\n",
    "mlb = pickle.load(open(\"mlb_domaine.pickle\", 'rb'))\n",
    "vectorizer = pickle.load(open('tfidf_domaine.pickle', 'rb'))\n",
    "X_test_vec = vectorizer.transform(new_data_preprocessed.corpus)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "#Afficher les résultats de prédiction\n",
    "y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "#for item,label in zip(new_data_preprocessed.corpus, y_pred_inversed):\n",
    "#    print ('%s => %s' % (item[0:15],label))\n",
    "\n",
    "    \n",
    "new_data_preprocessed['prediction'] = pd.Series(y_pred_inversed, index=new_data_preprocessed.index)\n",
    "new_data_pred = new_data_preprocessed.explode('prediction')\n",
    "#new_data_pred\n",
    "new_classes = new_data_pred.prediction.unique()\n",
    "\n",
    "for i in new_classes:\n",
    "    if os.path.isfile('tfidf_' + str(i) + '.pickle') and os.path.isfile(\"models/concept/svc_concept_\" + str(i) + \".pkl\") and os.path.isfile(\"mlb_\"+str(i)+\".pickle\"):\n",
    "        print(str(i))\n",
    "        dataset1 = new_data_pred.loc[new_data_pred['prediction'] == i]\n",
    "        dataset = dataset1.copy()\n",
    "        #dataset = dataset1.copy()\n",
    "        dataset.titre = dataset.titre.map(lambda e: word_tokenize(e, language = 'french'))\n",
    "        dataset.resume = dataset.resume.map(lambda e: word_tokenize(e, language = 'french'))\n",
    "        #dataset=preprocessing(dataset1, 'french', 'test')       \n",
    "        # concatenate tokens\n",
    "        dataset['titre'] = dataset.titre.map(lambda e: ' '.join(e))\n",
    "        dataset['resume'] = dataset.resume.map(lambda e: ' '.join(e)) \n",
    "        dataset['corpus'] = dataset['titre']  +' ' + dataset['resume']\n",
    "        #print(dataset.corpus)\n",
    "        vectorizer = pickle.load(open('tfidf_' + str(i) + '.pickle', 'rb'))\n",
    "        new_X = vectorizer.transform(dataset.corpus)\n",
    "        model = joblib.load(\"models/concept/svc_concept_\" + str(i) + \".pkl\")\n",
    "        pred = model.predict(new_X)\n",
    "        mlb = pickle.load(open(\"mlb_\"+str(i)+\".pickle\", 'rb'))\n",
    "        y_pred_inversed = mlb.inverse_transform(pred)\n",
    "        for item,label in zip(dataset.origine, y_pred_inversed):\n",
    "            for x in label:\n",
    "                r = requests.get('https://www.idref.fr/'+x+'.xml')\n",
    "                root = ET.fromstring(r.content)\n",
    "                for e in root.findall('./datafield[@tag=\"250\"]'):\n",
    "                    for f in e.findall('./subfield[@code=\"a\"]'):\n",
    "                        #print ('    %s => %s' % (item[0:200],f.text))\n",
    "                        df = df.append({'titre': item,'concept':f.text}, ignore_index=True)\n",
    "df.sort_values(by=['titre'])\n",
    "df=df.groupby(['titre'])['concept'].apply(','.join).reset_index()\n",
    "df.to_excel(\"output/prediction_concepts_newdata.xlsx\") \n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diabète en prison : mise en place d’un program...</td>\n",
       "      <td>Diabètes,Thérapeutique,Éducation des patients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L’autonomie du patient, d’une sémantique uniqu...</td>\n",
       "      <td>Éducation des patients,Relations médecin-patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               titre                                            concept\n",
       "0  Diabète en prison : mise en place d’un program...      Diabètes,Thérapeutique,Éducation des patients\n",
       "1  L’autonomie du patient, d’une sémantique uniqu...  Éducation des patients,Relations médecin-patie..."
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de=df.copy()\n",
    "de=de.groupby(['titre'])['concept'].apply(','.join).reset_index()\n",
    "de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2R3IHvc2lMvQ",
    "SxCOsSzPPOva",
    "Zgnf42sYXRaq",
    "ynEaXbGBlYFC",
    "4cGoPh3A_ly0",
    "oAGcuJYsIg0b",
    "O1DCw93zIdxg",
    "KTxYyEiK9x5m",
    "0y9a-VfrE0vo",
    "uXvByC42mhwb",
    "6_DYOC_zFGf3",
    "sOdROxG9KHZj"
   ],
   "machine_shape": "hm",
   "name": "CARAML-prétraitement, classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
