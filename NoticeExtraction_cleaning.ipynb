{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning ABES extraction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook permet de vérifier l'extraction de données réalisée par l'ABES et d'enlever les notices potentiellement mal formattées (du fait de correspondance entre les métadonnées de la notice et le type de séparateur choisi par exemple).\n",
    "Une étape de mise à plat des chaines d'indexation est également réalisée ainsi qu'une première exploration des concepts RAMEAU et labels TEF (issus de la classification décimale de DEWEY) sous forme de visualisation graphique (barplots et wordcloud)\n",
    "\n",
    "MAJ - 03/05/2023 (Aurélie Thébault - EcoStats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres graphiques\n",
    "%matplotlib inline\n",
    "rc = {\n",
    "    'font.size': 14,\n",
    "    'font.family': 'Arial',\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'figure.max_open_warning': 30}\n",
    "\n",
    "sns.set(font='Arial', rc=rc)\n",
    "sns.set_style(\n",
    "    \"whitegrid\", {\n",
    "        'axes.edgecolor': 'k',\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': True,\n",
    "        'xtick.major.width': 1,\n",
    "        'ytick.major.width': 1\n",
    "        })\n",
    "sns.set_context(\n",
    "    \"notebook\",\n",
    "    font_scale=1.1,\n",
    "    rc={\"lines.linewidth\": 1.5})\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "path = \".\"\n",
    "os.chdir(path)\n",
    "data_path = path + \"\\\\data\"\n",
    "output_path = path + \"\\\\outputs\"\n",
    "fig_path = path + \"\\\\figs\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(filename, encoding=\"utf-8\", plot=False):\n",
    "    with open(\n",
    "        os.path.join(data_path, filename), 'r',\n",
    "            newline='', encoding=encoding) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t',)\n",
    "        data = list(csv_reader)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class checkDataFormat:\n",
    "    def __init__(self, df_list):\n",
    "        self.df = df_list\n",
    "\n",
    "    def check_format(self, plot=False):\n",
    "        # Define dimension\n",
    "        df = self.df\n",
    "\n",
    "        nb_notice = len(df)\n",
    "\n",
    "        # Check number of column\n",
    "        len_col = []\n",
    "        for row in df:\n",
    "            len_col.append(len(row))\n",
    "        max_number_col = max(len_col)\n",
    "        print(f\"There are {nb_notice} in this file with till {max_number_col} columns\")\n",
    "\n",
    "        if plot:\n",
    "            # Show distribution\n",
    "            sns.histplot(len_col)\n",
    "\n",
    "        self.nb_notice = nb_notice\n",
    "        self.max_number_col = max_number_col\n",
    "        self.len_col = len_col\n",
    "\n",
    "    def sort_notices(self, official_nb_col=5, save_file=\"working_data.csv\", export_name=None):\n",
    "        \n",
    "        # Checking and removing badly formatted notices\n",
    "        len_col = self.len_col\n",
    "        df = self.df\n",
    "\n",
    "        bad_formated_notices = [x for x in len_col if x != official_nb_col]\n",
    "        print(f\"There are {len(bad_formated_notices)} badly formatted notices\")\n",
    "\n",
    "        ids_to_keep = [True if x == official_nb_col else False for x in len_col]\n",
    "        print(f\"There are {sum(ids_to_keep)-1} well formatted rows\")\n",
    "\n",
    "        cleaned_data = [row for row, id in zip(df, ids_to_keep) if id]\n",
    "        cleaned_data = pd.DataFrame(cleaned_data[1:], columns=cleaned_data[0])\n",
    "        print(f\" ==> Working dataset has {len(cleaned_data)} notices\")\n",
    "\n",
    "        # Verification du format des données\n",
    "        print(f\"Cleaned file contains {cleaned_data.shape[0]} notices and  {cleaned_data.shape[1]} columns\")\n",
    "\n",
    "        data_to_check = [row for row, id in zip(data, ids_to_keep) if not id]\n",
    "        data_to_check = pd.DataFrame(data_to_check)\n",
    "        print(f\"Need to check {len(data_to_check)} notices extractions\")\n",
    "\n",
    "        # Sauvegarde des données\n",
    "        cleaned_data.to_csv(\n",
    "            os.path.join(data_path, save_file),\n",
    "            index=0)\n",
    "        print(f\"Save working data as {save_file}\")\n",
    "\n",
    "        if export_name:\n",
    "        # Export des données à vérifier\n",
    "            data_to_check.to_csv(\n",
    "                os.path.join(data_path, export_name),\n",
    "                index=0)\n",
    "            print(f\"Save data to check data as {export_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barplot_of_tags(\n",
    "    tags_list,\n",
    "    nb_of_tags,\n",
    "    xlabel=\"Nombre d'occurences\",\n",
    "    ylabel=\"\",\n",
    "    figsave=None,\n",
    "    figsize=(10, 30),\n",
    "):\n",
    "    \"\"\"\n",
    "    Description: plot barplot of tags count (descending order) from a list of tags\n",
    "\n",
    "    Arguments:\n",
    "        - tags_list (lsit): list of tags\n",
    "        - nb_of_tags (int) : number of tags to plot in barplot (default=50)\n",
    "        - xlabel, ylabel (str): labels of the barplot\n",
    "        - figsize (list) : figure size (default : (10, 30))\n",
    "\n",
    "    Returns :\n",
    "        - Barplot of nb_of_tags most important tags\n",
    "\n",
    "    \"\"\"\n",
    "    tag_count = Counter(tags_list)\n",
    "    tag_count_sort = dict(tag_count.most_common(nb_of_tags))\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(\n",
    "        x=list(tag_count_sort.values()),\n",
    "        y=list(tag_count_sort.keys()),\n",
    "        orient=\"h\",\n",
    "        palette=\"viridis\",\n",
    "    )\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if figsave:\n",
    "        plt.savefig(figsave, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list):\n",
    "    flat_list = [item for sublist in list for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(keywords, save_file=None):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    wordcloud = WordCloud(\n",
    "        width=1000, height=500,\n",
    "        background_color='white').generate_from_frequencies(Counter(keywords))\n",
    "    plt.imshow(wordcloud)\n",
    "    if save_file:\n",
    "        plt.savefig(os.path.join(fig_path, save_file), dpi=300, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class removeVedettes:\n",
    "    def __init__(self, df, col_name, vedette_list):\n",
    "        self.df = df\n",
    "        self.col_name = col_name\n",
    "        self.vedettes = vedette_list\n",
    "\n",
    "    def remove_vedette(self):\n",
    "        df = self.df\n",
    "        col = self.col_name\n",
    "        vedettes = self.vedettes\n",
    "\n",
    "        is_ved = np.zeros(df.shape[0])\n",
    "        for ved in vedettes:\n",
    "            res = df[col].apply(lambda x: ved in x)\n",
    "            print(f\"Nbre de notices contenant le concept '{ved}' : {sum(res)}\")\n",
    "            is_ved += res\n",
    "\n",
    "        # Reduction du jeu de données\n",
    "        df_reduced = self.df[(is_ved) == 0]\n",
    "        print(f\"Les vedettes de {vedettes} ont été retirées du dataset\")\n",
    "        print(f\"Le dataset contient maintenant {df_reduced.shape[0]} notices\")\n",
    "\n",
    "        self.df_reduced = df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_from_ddc(ddc):\n",
    "    # Get Domain according to Dewey code\n",
    "    pattern_tenth = re.compile(r'\\d{2}0|00[0-9]|944|796')\n",
    "    pattern_unit= re.compile(r'\\d{2}[1-9]')\n",
    "    if re.findall(pattern_tenth, ddc):\n",
    "        tef = str(re.findall(pattern_tenth, ddc)[0])\n",
    "    elif re.findall(pattern_unit, ddc):\n",
    "        tef = str(re.findall(pattern_unit, ddc)[0][:-1] + str(0))\n",
    "    else:\n",
    "        tef = None\n",
    "    return tef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des données\n",
    "filepath = \"extraction\\export.dsv\"\n",
    "working_data_filename = \"working_data.csv\"\n",
    "export_data_filename = \"data_to_check.csv\"\n",
    "encoding = \"latin-1\"  # (useful only on first extraction)\n",
    "data = import_data(filepath, encoding)\n",
    "filename = filepath.split('\\\\')[-1].split('.')[0]\n",
    "merge_with_dewey = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des données\n",
    "filepath = \"extraction\\export_sans_dewey.dsv\"\n",
    "working_data_filename = \"working_data_sans_dewey.csv\"\n",
    "export_data_filename = \"data_to_check_sans_dewey.csv\"\n",
    "# encoding = \"latin-1\"  # (useful only on first extraction)\n",
    "data = import_data(filepath)#, encoding)\n",
    "filename = filepath.split('\\\\')[-1].split('.')[0]\n",
    "merge_with_dewey = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean file\n",
    "df = checkDataFormat(data)\n",
    "df.check_format()\n",
    "df.sort_notices(\n",
    "    save_file=working_data_filename,\n",
    "    export_name=export_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load working data\n",
    "df = pd.read_csv(os.path.join(data_path, working_data_filename))\n",
    "print(f\"Data loaded :\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'une colonne description (Titre + resumé)\n",
    "df.loc[:, \"DESCR\"] = df.loc[:, 'TITRE'] + ' ' + df.loc[:, 'RESUME']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore RAMEAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des indices contenant \" -- \" dans la colonne RAMEAU => i.e chaines d'indexation\n",
    "df[\"presence_chaine_indexation\"] = df[\"RAMEAU\"].apply(lambda x: True if re.search(' -- ', x) else False)\n",
    "n_chaine_index = df[\"presence_chaine_indexation\"].sum()\n",
    "print(f\"Le jeu de données contient {n_chaine_index} notices avec des chaines d'indexation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"presence_chaine_indexation\"] == True, [\"PPN\", \"RAMEAU\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des chaines d'indexation\n",
    "df[\"rameau_chaines_index\"] = df[\"RAMEAU\"].apply(lambda x: x.split(';'))\n",
    "print(df.loc[1:10, \"rameau_chaines_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de de notices d'autorité différentes (y compris pré-construites)\n",
    "# ex d'autorité preconstruite : Science-fiction américaine -- Traductions française\n",
    "from itertools import chain\n",
    "rameau_chaine_index = df[\"rameau_chaines_index\"].tolist()\n",
    "rameau_list_chaines_index = list(chain(*rameau_chaine_index))\n",
    "print(f\"{len(rameau_list_chaines_index)} chaines d'indexation rameau, dont {len(set(rameau_list_chaines_index))} différentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot_of_tags(\n",
    "    tags_list=rameau_list_chaines_index,\n",
    "    nb_of_tags=20,\n",
    "    xlabel=\"Nombre de references\",\n",
    "    ylabel=\"RAMEAU - Chaines d'indexation\",\n",
    "    figsave=os.path.join(fig_path, str(filename + \"_\" + 'barplot_Rameau_ChainesIndex.png')),\n",
    "    figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mettre à plat TOUS les mots clé\n",
    "pattern = r';\\s*(?![^()]*\\))| -- '\n",
    "df[\"rameau_concepts\"] = df[\"RAMEAU\"].apply(lambda x: re.split(pattern, x))\n",
    "df.loc[1:20, [\"RAMEAU\", \"rameau_concepts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = flatten(df['rameau_concepts'])\n",
    "print(f\"Le dataset contient {len(set(keywords))} concepts RAMEAU differents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show main vedettes\n",
    "plot_wordcloud(keywords, save_file=str(filename + \"_\" + \"rameau_concepts_wordcloud.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot_of_tags(\n",
    "    tags_list=keywords,\n",
    "    nb_of_tags=20,\n",
    "    xlabel=\"Nombre de references\",\n",
    "    ylabel=\"RAMEAU - Concepts\",\n",
    "    figsave=os.path.join(fig_path, str(filename + \"_\" + 'barplot_Rameau_concepts.png')),\n",
    "    figsize=(8, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove vedettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vedettes = [\"Ouvrages pour la jeunesse\", \"Roman pour la jeunesse\"]\n",
    "colonne = \"rameau_concepts\"\n",
    "temp = removeVedettes(df, colonne, list_vedettes)\n",
    "temp.remove_vedette()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset final\n",
    "cleaned_df = temp.df_reduced\n",
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show main vedettes\n",
    "keywords2 = flatten(cleaned_df['rameau_concepts'])\n",
    "plot_wordcloud(keywords2, save_file=str(filename + \"_\" + \"rameau_concepts_wordcloud_cleaned.png\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dewey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_with_dewey:\n",
    "    # Find DDC domain\n",
    "    cleaned_df[\"DDC\"] = cleaned_df[\"DEWEY\"].apply(lambda x: get_domain_from_ddc(x))\n",
    "    # Merge with TEF labels\n",
    "    ddc = pd.read_csv(os.path.join(data_path, \"dewey_label.csv\"), index_col=0, dtype=str)\n",
    "\n",
    "    # Merge \n",
    "    cleaned_df = cleaned_df.merge(ddc, on=\"DDC\", how='left')\n",
    "    print(f\"Dimension of the dataframe with TEF labels: {cleaned_df.shape}\")\n",
    "    print(\"Column headers: \", list(cleaned_df.columns))\n",
    "\n",
    "    # Visualization\n",
    "    plot_barplot_of_tags(\n",
    "        tags_list=cleaned_df[\"TEF_LABEL\"],\n",
    "        nb_of_tags=20,\n",
    "        xlabel=\"Nombre de references\",\n",
    "        ylabel=\"Libellés TEF\",\n",
    "        figsave=os.path.join(fig_path, 'barplot_libelles_TEF.png'),\n",
    "        figsize=(8, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save working file as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving file\n",
    "cleaned_df.to_csv(os.path.join(data_path, working_data_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abes_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
